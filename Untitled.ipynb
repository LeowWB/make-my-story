{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complicated-representative",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.corpus\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from pdb import set_trace as st\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bright-question",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstopwords = stopwords + [\\n    \"n\\'t\",\\n    \\'not\\',\\n    \\'mr\\',\\n    \\'mr.\\'\\n]\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "'''\n",
    "stopwords = stopwords + [\n",
    "    \"n't\",\n",
    "    'not',\n",
    "    'mr',\n",
    "    'mr.'\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "western-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('train.csv', newline='', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, dialect='excel')\n",
    "    train_set = list(reader)\n",
    "\n",
    "train_set = list(map(\n",
    "    lambda x: [int(x[0]), x[1], int(x[2])],\n",
    "    train_set[1:]\n",
    "))\n",
    "\n",
    "random.shuffle(train_set)\n",
    "\n",
    "for datum in train_set:\n",
    "    if datum[2] == -1:\n",
    "        datum.append([1,0,0])\n",
    "    elif datum[2] == 0:\n",
    "        datum.append([0,1,0])\n",
    "    else:\n",
    "        datum.append([0,0,1])\n",
    "\n",
    "val_set = train_set[:2000]\n",
    "train_set = train_set[2000:]\n",
    "\n",
    "with open('test.csv', newline='', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, dialect='excel')\n",
    "    test_set = list(reader)\n",
    "\n",
    "test_set = list(map(\n",
    "    lambda x: (int(x[0]), x[1]),\n",
    "    test_set[1:]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "included-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_tokens(string):\n",
    "    tokens = word_tokenize(string)\n",
    "    tokens = filter(\n",
    "        lambda x: re.match(re.compile(r\"^[A-Za-z'][A-Za-z'.]*$\"), x),\n",
    "        tokens\n",
    "    )\n",
    "    tokens = map(\n",
    "        lambda x: x.lower(),\n",
    "        tokens\n",
    "    )\n",
    "    tokens = filter(\n",
    "        lambda x: x not in stopwords and x[0] != \"'\",\n",
    "        tokens\n",
    "    )\n",
    "    return list(tokens)\n",
    "\n",
    "def remove_unknown_words(vocab, tokens):\n",
    "    return list(filter(\n",
    "        lambda x: x in vocab,\n",
    "        tokens\n",
    "    ))\n",
    "\n",
    "def generate_word_counts(train_set):\n",
    "    word_counts = {}\n",
    "    for train_datum in train_set:\n",
    "        for word in string_to_tokens(train_datum[1]):\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    return word_counts\n",
    "\n",
    "def remove_rare_words_from_word_counts(word_counts, threshold=5):\n",
    "    new_dict = {}\n",
    "    for word in word_counts.keys():\n",
    "        if word_counts[word] >= threshold:\n",
    "            new_dict[word] = word_counts[word]\n",
    "    return new_dict\n",
    "\n",
    "# returns a dict that maps words to integers. will be used for encoding text.\n",
    "def generate_wordmap(words):\n",
    "    wordmap = {}\n",
    "    for i in range(len(words)):\n",
    "        wordmap[words[i]] = i\n",
    "    return wordmap\n",
    "\n",
    "def string_to_vector(string, wordmap):\n",
    "    tokens = string_to_tokens(string)\n",
    "    tokens = remove_unknown_words(wordmap.keys(), tokens)\n",
    "    result = np.zeros(len(wordmap))\n",
    "    for token in tokens:\n",
    "        result[wordmap[token]] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "peaceful-capacity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10464\n"
     ]
    }
   ],
   "source": [
    "word_counts = generate_word_counts(train_set)\n",
    "print(len(word_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "laden-bedroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x238430ef940>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAa00lEQVR4nO3de5BcZ53e8e/T3XPRjG4jaSxkWbJkIy8xWWMbYczCEjZefMvFUEUo8YdRsaREErsKqjZVMbuVQHaLFLkAFSfElFkUm12C17tcrFBKbK2BoijiiwyyLNsIj29Ysi5jy5Js3ebSv/xx3h6d6Z7R6DIz3Tr9fKrGffo9p0//+njmOa/ec/ocRQRmZtYeSs0uwMzMZo9D38ysjTj0zczaiEPfzKyNOPTNzNpIpdkFnMqSJUti1apVzS7DzOy88sQTT7wWEf0TzWvp0F+1ahVbt25tdhlmZucVSS9PNs/DO2ZmbcShb2bWRhz6ZmZtxKFvZtZGHPpmZm1kytCXtELSTyQ9I+lpSZ9N7V+UtFvStvRzc+41n5c0IGmnpBty7TemtgFJd8zMRzIzs8mczimbI8AfR8QvJc0DnpC0Jc37WkT8l/zCki4H1gHvBC4E/k7SZWn214EPA7uAxyVtiohnpuODmJnZ1Kbs6UfEnoj4ZZp+E3gWWH6Kl9wC3BcRJyLiRWAAuCb9DETECxExBNyXlp12R06M8NWHdrLtlYMzsXozs/PWGY3pS1oFXAU8mppul7Rd0kZJfaltOfBK7mW7Uttk7fXvsUHSVklbBwcHz6S8MceGR7nzxwNs33XwrF5vZlZUpx36kuYC3wM+FxGHgbuAS4ErgT3AV6ajoIi4OyLWRsTa/v4Jv0U8da1j65qOiszMiuO0LsMgqYMs8L8TEd8HiIh9ufnfBH6Unu4GVuReflFq4xTt00rKYt93BTMzG+90zt4R8C3g2Yj4aq59WW6xjwI70vQmYJ2kLkmrgTXAY8DjwBpJqyV1kh3s3TQ9H6Ou5vToyDczG+90evrvB24FnpK0LbX9CfAJSVeSZetLwGcAIuJpSfcDz5Cd+XNbRIwCSLodeBAoAxsj4ulp+yQ5qaPv4R0zszpThn5E/JyTnee8zad4zZeAL03QvvlUr5sumrBcMzMr9Ddy3dE3MxuvmKE/Nrzj2Dczyytk6MujO2ZmEypm6KdHd/TNzMYrZujXztP3qL6Z2TjFDP306J6+mdl4xQz92oHc5pZhZtZyihn61C7D0ORCzMxaTDFDf6yn79Q3M8srZOjXuKdvZjZeIUPf5+mbmU2smKGPL61sZjaRYoa+e/pmZhMqZOjXuKNvZjZeIUPfN1ExM5tYMUNfPk/fzGwixQz99Ojz9M3Mxitm6Pt2iWZmEypo6NeusmlmZnmFDP0x7uqbmY1T2NCX3NM3M6tX3NDHHX0zs3rFDX3JZ++YmdUpbujjnr6ZWb3ihr6vv2Nm1qCwoQ8+kGtmVq+woS/k4R0zszqFDX3kyzCYmdUrbOgLPL5jZlanuKHvL2eZmTWYMvQlrZD0E0nPSHpa0mdT+yJJWyQ9lx77Ursk3SlpQNJ2SVfn1rU+Lf+cpPUz97FqY/qOfTOzvNPp6Y8AfxwRlwPXArdJuhy4A3g4ItYAD6fnADcBa9LPBuAuyHYSwBeA9wLXAF+o7ShmguTz9M3M6k0Z+hGxJyJ+mabfBJ4FlgO3APemxe4FPpKmbwG+HZlHgIWSlgE3AFsi4kBEvAFsAW6czg+TJzy8Y2ZW74zG9CWtAq4CHgWWRsSeNGsvsDRNLwdeyb1sV2qbrL3+PTZI2ipp6+Dg4JmUV78e9/TNzOqcduhLmgt8D/hcRBzOz4ts8HxaIjYi7o6ItRGxtr+//6zXk/X0nfpmZnmnFfqSOsgC/zsR8f3UvC8N25Ae96f23cCK3MsvSm2Ttc8Mj+mbmTU4nbN3BHwLeDYivpqbtQmonYGzHngg1/7JdBbPtcChNAz0IHC9pL50APf61DYjfOkdM7NGldNY5v3ArcBTkraltj8BvgzcL+nTwMvAx9O8zcDNwABwFPgUQEQckPTnwONpuT+LiAPT8SHMzOz0TBn6EfFzJu84XzfB8gHcNsm6NgIbz6TAs5UdyPX4jplZnr+Ra2bWRoob+vhArplZveKGvm+XaGbWoLihj3v6Zmb1ihv6HtM3M2tQ2NDHd84yM2tQ2NCX76JiZtaguKGPx/TNzOoVN/R97R0zswbFDX18yqaZWb3ihr6vuGZm1qCwoQ8e3jEzq1fY0PftEs3MGhU39H27RDOzBoUNffDtEs3M6hU29OXxHTOzBoUOfWe+mdl4xQ19fOcsM7N6xQ199/TNzBoUN/TxefpmZvWKG/qSe/pmZnWKG/rgMX0zszqFDX187R0zswbFDX18INfMrF5hQ983zjIza1TY0C9JVD2mb2Y2TmFD33fOMjNrVNjQL8l3zjIzq1fY0AeoOvPNzMYpbOiXfD19M7MGhQ39bEzfqW9mljdl6EvaKGm/pB25ti9K2i1pW/q5OTfv85IGJO2UdEOu/cbUNiDpjun/KOOVfBkGM7MGp9PTvwe4cYL2r0XElelnM4Cky4F1wDvTa/6HpLKkMvB14CbgcuATadkZUxI+ZdPMrE5lqgUi4meSVp3m+m4B7ouIE8CLkgaAa9K8gYh4AUDSfWnZZ8685NMk+UCumVmdcxnTv13S9jT805falgOv5JbZldoma28gaYOkrZK2Dg4OnnVxJY/pm5k1ONvQvwu4FLgS2AN8ZboKioi7I2JtRKzt7+8/6/X4evpmZo2mHN6ZSETsq01L+ibwo/R0N7Ait+hFqY1TtM8IfznLzKzRWfX0JS3LPf0oUDuzZxOwTlKXpNXAGuAx4HFgjaTVkjrJDvZuOvuyT6dGqFZn8h3MzM4/U/b0JX0X+BCwRNIu4AvAhyRdSXYdy5eAzwBExNOS7ic7QDsC3BYRo2k9twMPAmVgY0Q8Pd0fpq5u9/TNzOqcztk7n5ig+VunWP5LwJcmaN8MbD6j6s6B8GUYzMzqFfYbuSXJ19M3M6tT2NCXv5xlZtagsKHvyzCYmTUqbOi7p29m1qjAoe/LMJiZ1Sts6Jf8lVwzswaFDX2fsmlm1qiwoe/LMJiZNSps6PsyDGZmjQoc+j5l08ysXnFDH19P38ysXmFDvyT55B0zszqFDX1/OcvMrFFhQ9+XYTAza1TY0Mc9fTOzBoUNfV9a2cysUYFD3z19M7N6hQ19X4bBzKxRYUPfl2EwM2tU2NDHl2EwM2tQ2NAvSc0uwcys5RQ29LMxfQ/vmJnlFTb0fRkGM7NGhQ19X4bBzKxRgUPfl2EwM6tX4ND3pZXNzOoVNvQ7SmJ41KFvZpZX2NDv7ihzfHi02WWYmbWUwoZ+V0eZEyNVD/GYmeUUN/Qr2Uc7MeKv5ZqZ1RQ29Ls7ygCcGHbom5nVTBn6kjZK2i9pR65tkaQtkp5Lj32pXZLulDQgabukq3OvWZ+Wf07S+pn5OCd1d2Qf7fiIx/XNzGpOp6d/D3BjXdsdwMMRsQZ4OD0HuAlYk342AHdBtpMAvgC8F7gG+EJtRzFTuitZT98Hc83MTpoy9CPiZ8CBuuZbgHvT9L3AR3Lt347MI8BCScuAG4AtEXEgIt4AttC4I5lWteGd4x7eMTMbc7Zj+ksjYk+a3gssTdPLgVdyy+1KbZO1N5C0QdJWSVsHBwfPsryTB3Ld0zczO+mcD+RGdk7ktJ0XGRF3R8TaiFjb399/1usZO5Drs3fMzMacbejvS8M2pMf9qX03sCK33EWpbbL2GTN2INc9fTOzMWcb+puA2hk464EHcu2fTGfxXAscSsNADwLXS+pLB3CvT20z5uSYvkPfzKymMtUCkr4LfAhYImkX2Vk4Xwbul/Rp4GXg42nxzcDNwABwFPgUQEQckPTnwONpuT+LiPqDw9Pq5CmbHt4xM6uZMvQj4hOTzLpugmUDuG2S9WwENp5Rdeegy6dsmpk1KOw3crs6fBkGM7N6hQ39k5dhcE/fzKymuKHv4R0zswaFDf2OsijJ38g1M8srbOhLoqviG6mYmeUVNvQhO23TB3LNzE4qeOi7p29mllf80HdP38xsTKFDv6tSck/fzCyn2KHv4R0zs3EKHfrdFR/INTPLK3bod5T9jVwzs5yCh37JX84yM8spdOh3VcocH3FP38ysptChn/X0HfpmZjWFDv2ezgrHhhz6ZmY1BQ/9MkeHRsnu7WJmZoUO/d6uCiPVYGjUB3PNzKDgod/TmV1T/+gJD/GYmUHBQ7+3M7sF8JGhkSZXYmbWGgod+j1dqafvg7lmZkDBQ7/W03/rhHv6ZmZQ8NB/24JuAF567UiTKzEzaw2FDv3Lls6jXBID+99qdilmZi2h0KFfLom+nk7eODrU7FLMzFpCoUMfYHFvJ6+/5dA3M4M2CP2+3g739M3MksKH/uLeLl4/4tA3M4M2CP3lfXPYdeCYT9s0M6MNQv93ly9gaLTK7jeONbsUM7OmO6fQl/SSpKckbZO0NbUtkrRF0nPpsS+1S9KdkgYkbZd09XR8gKnM6ci+levr6puZTU9P/w8i4sqIWJue3wE8HBFrgIfTc4CbgDXpZwNw1zS895S6U+j7BulmZjMzvHMLcG+avhf4SK7925F5BFgoadkMvP843R3ZR3RP38zs3EM/gIckPSFpQ2pbGhF70vReYGmaXg68knvtrtQ2o2o9fV90zcwMKuf4+g9ExG5JFwBbJP06PzMiQtIZ3bYq7Tw2AKxcufIcyzt5/Z1XD/pArpnZOfX0I2J3etwP/AC4BthXG7ZJj/vT4ruBFbmXX5Ta6td5d0SsjYi1/f3951IekH0jd15XhZde90XXzMzOOvQl9UqaV5sGrgd2AJuA9Wmx9cADaXoT8Ml0Fs+1wKHcMNCMkcTq/l5eGHTom5mdy/DOUuAHkmrr+V8R8X8lPQ7cL+nTwMvAx9Pym4GbgQHgKPCpc3jvM/I7S+fx0DP7iAhSvWZmbemsQz8iXgDeNUH768B1E7QHcNvZvt+5uOKiBfzNE7vY9cYxVizqaUYJZmYtofDfyAW49IK5AAwM+rr6Ztbe2iL037lsASXB1pcONLsUM7OmaovQX9DTwZUrFrL5qb0Mj/qbuWbWvtoi9AH++e9fwouvHeFnvxlsdilmZk3TNqH/e5cuBuBF3yTdzNpY24T+wp5O5ndXeN7n65tZG2ub0Af4/TX9PLBtN4eODje7FDOzpmir0F93zQqODo2y49VDzS7FzKwp2ir03/G2+QBse+VgcwsxM2uStgr9/nldvO+Sxdzzi5d8fX0za0ttFfoAn/3DNQy+eYIf/qrhAp9mZoXXdqH/3tWLmNdV4RfPv97sUszMZl3bhb4k/vG7lrHpyVfZ7RurmFmbabvQB/jMBy9Fgq88tLPZpZiZzaq2DP1VS3q5YvkCfrpzkGr1jO7maGZ2XmvL0AdY/3urOHBkiL985OVml2JmNmvaNvQ/etVy/sFl/fyHzc/6vH0zaxttG/qS+M//7Ao6yyU+/o3/x1O7/C1dMyu+tg19gAvmdXPfZ65lQU8H/+KvnuDQMV+Tx8yKra1DH+CdFy7g7lvfzd7Dx1l39yPs2O0ev5kVV9uHPsBVK/u4c91V7Dt8nH/y33/Ov/3hDo4OjTS7LDOzaVdpdgGt4h9dsYxrVi/ii//7ab7z6Mv89Df7WfeelfzR+1czp7Pc7PLMzKaFIlr3PPW1a9fG1q1bZ/19f7pzP1/d8hu27zrE2+Z3c+v7LuafvutCVizqmfVazMzOlKQnImLthPMc+pP74a92c9dPn2fnvjcpCW77g7dz6/su5oJ53U2rycxsKg79c/Tb149yx/e384vnX0eC96xaxOeuW8P7Ll2MpGaXZ2Y2jkN/mvzqt2/w41/v57uPvcJrb53gwgXdvHvVIj58+VI+8PYlLOrtbHaJZmYO/el26NgwP9r+Kj9/7jV+PvAabx4fQYKrV/bx/rcv4d0X9/Hui/uY2+Xj5GY2+xz6M+j48CjbXjnILwZeY/OOvTw/+Ba1TbpyUQ+rl/SyanEP71g2n6tX9nFJfy8dZZ8pa2Yzx6E/iw4dHebJXQd54uU3eGr3IV49eIzfHjjK0aHs9owdZfH3ls3nd5bOY9mCbpb3zeHChXPo6+lk5eIe5nd3NPkTmNn57lSh7/GHabagp4MPXtbPBy/rH2sbrQY7977JM3sOs3PvYZ7cdYgf/3o/B44OUb/PndddYVFvJ0vmdjG/u8LiuV10VUr09XSyqLeTCxd2c8H8bhb1dNLVUWJRbyddFX+PwMxOj0N/FpRL4vIL53P5hfPHtR8fHmXf4ePsOXSc1946wcuvH2XwzRPsO3ycw8eHefXgcXbufZMTI1VePzI04bolWJx2EnM6y1wwr4uezmzH0dtVobMs5nZVWNjTSWelRFelxJK5XXSUS3RWxJzOCot6Oukoi3JJPhvJrOBmPfQl3Qj8V6AM/EVEfHm2a2gV3R1lLl7cy8WLe6dcdni0yuFj2Y7g1UPHODo0wrGhKnsPH+fVg8c4fGyYQ8eGeWHwCEeHRhl86wRDI9UzqkeCBXM66O2sUCmLSknZ864KlZIol0p0lMW87go9nRXKJaX27GfBnA66O8pjbZWymNNRZv6cDsrK2koljU3P664wp6M81laSKJWynWRJorNcolTyTshsOs1q6EsqA18HPgzsAh6XtCkinpnNOs5HHeUSi+d2sXhuF7970YLTek1EMDwaHDgyxNGhEYZHg8PHhzl8bJjh0SrDo8HBY8McOTHC8Eh17F8UQyNVRqpVhkervPbWEG+dGGFkNBipBiOjVQ4cGWJ4tMpoNRiNYLSavc90q5TEnM7y2E4g+zm5U5jXXaG7o0xJ2aWya4+CsR2IEErtlZJYOKcjt+z4+dnz7LW16ZPt2WNvZ5nuznK2DOPXwWSvrVt2TmeJ3s7K+PY0j/xrJ1i/0n/G6k7Lzuko09NVHlum9i+22i5zbP1pmlw7ufcb11b/vrn3Hr/c+PdbMKfDO+sWNts9/WuAgYh4AUDSfcAtgEN/BkiisyLetmDmv0Ecke1cajuCkdFsZ3Dw2DDHhkappp3DaATVarYDeePIECPVoJraRgOq6floBAePDjM0UiXS82qaP5p+DhzN3g+gGkFE9lgNxnZE1QgCqAacGB5l5943x+qtBgTZ6yK11aZr64v0+tr0kXRA3k6tfmeRHzacaOdSP7N+mfxyaly8YSc38bpyNUwyr7tSordFTrV+x7L5/LdPXDXt653tT7cceCX3fBfw3vwCkjYAGwBWrlw5e5XZOZHE4rldzS5jxg2NVMftYMZ2FmQ7BiZpz08fSv/Siol2OrX1jGvP3nvc+shm5tc5Uq2m5dLyxNjzk23ZemrTtYn8sieXm2A9Y6+JxuUi25kfTveliPGLNqxrXA0TvB/jXnfyPRqXn/p98uo/f36Z/HZsthV9c2Zkva2xS8uJiLuBuyE7ZbPJ5ZiN01k59+9YzMa/vMwmM9vfEtoNrMg9vyi1mZnZLJjt0H8cWCNptaROYB2waZZrMDNrW7M6vBMRI5JuBx4kO2VzY0Q8PZs1mJm1s1kf04+IzcDm2X5fMzPzPXLNzNqKQ9/MrI049M3M2ohD38ysjbT09fQlDQIvn8MqlgCvTVM5M+V8qBHOjzpd4/Q5H+o8H2qE5tR5cUT0TzSjpUP/XEnaOtmNBFrF+VAjnB91usbpcz7UeT7UCK1Xp4d3zMzaiEPfzKyNFD307252AafhfKgRzo86XeP0OR/qPB9qhBars9Bj+mZmNl7Re/pmZpbj0DczayOFDH1JN0raKWlA0h1NruUlSU9J2iZpa2pbJGmLpOfSY19ql6Q7U93bJV09g3VtlLRf0o5c2xnXJWl9Wv45SetnocYvStqdtuc2STfn5n0+1bhT0g259hn9fZC0QtJPJD0j6WlJn03tLbM9T1Fjy2xPSd2SHpP0ZKrx36f21ZIeTe/31+my7EjqSs8H0vxVU9U+w3XeI+nF3La8MrU35e9nUtl9QYvzQ3bJ5ueBS4BO4Eng8ibW8xKwpK7tPwF3pOk7gP+Ypm8G/g/ZrTuvBR6dwbo+CFwN7DjbuoBFwAvpsS9N981wjV8E/vUEy16e/l93AavT70B5Nn4fgGXA1Wl6HvCbVE/LbM9T1Ngy2zNtj7lpugN4NG2f+4F1qf0bwL9M0/8K+EaaXgf89alqn8b/35PVeQ/wsQmWb8rfz2Q/Rezpj918PSKGgNrN11vJLcC9afpe4CO59m9H5hFgoaRlM1FARPwMOHCOdd0AbImIAxHxBrAFuHGGa5zMLcB9EXEiIl4EBsh+F2b89yEi9kTEL9P0m8CzZPeDbpnteYoaJzPr2zNtj7fS0470E8A/BP42tddvx9r2/VvgOkk6Re3T4hR1TqYpfz+TKWLoT3Tz9VP9cs+0AB6S9ISym74DLI2IPWl6L7A0TTe79jOtq1n13p7+mbyxNmTSKjWmIYaryHp/Lbk962qEFtqeksqStgH7yULweeBgRIxM8H5jtaT5h4DFM13jRHVGRG1bfilty69J6qqvs66epvz9FDH0W80HIuJq4CbgNkkfzM+M7N95LXfebKvWBdwFXApcCewBvtLUanIkzQW+B3wuIg7n57XK9pygxpbanhExGhFXkt0/+xrgHc2sZzL1dUr6+8Dnyep9D9mQzb9pXoWTK2Lot9TN1yNid3rcD/yA7Bd5X23YJj3uT4s3u/YzrWvW642IfekPrgp8k5P/bG9qjZI6yML0OxHx/dTcUttzohpbdXtGxEHgJ8D7yIZDanf5y7/fWC1p/gLg9dmqsa7OG9MQWkTECeB/0iLbsl4RQ79lbr4uqVfSvNo0cD2wI9VTO1K/HnggTW8CPpmO9l8LHMoND8yGM63rQeB6SX1pWOD61DZj6o5xfJRse9ZqXJfO6FgNrAEeYxZ+H9I48reAZyPiq7lZLbM9J6uxlbanpH5JC9P0HODDZMcefgJ8LC1Wvx1r2/djwI/Tv6gmq31aTFLnr3M7eJEdd8hvy5b4+wGKd/ZOnDxa/huy8cA/bWIdl5CdRfAk8HStFrJxx4eB54C/AxbFybMCvp7qfgpYO4O1fZfsn/PDZGOJnz6buoA/IjtQNgB8ahZq/MtUw3ayP6ZlueX/NNW4E7hptn4fgA+QDd1sB7aln5tbaXueosaW2Z7AFcCvUi07gH+X+zt6LG2TvwG6Unt3ej6Q5l8yVe0zXOeP07bcAfwVJ8/wacrfz2Q/vgyDmVkbKeLwjpmZTcKhb2bWRhz6ZmZtxKFvZtZGHPpmZm3EoW9m1kYc+mZmbeT/AyAVJOlcuVU0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_counts = remove_rare_words_from_word_counts(word_counts)\n",
    "print(len(word_counts.keys()))\n",
    "plt.plot(sorted(word_counts.values(), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adapted-garbage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"n't\", 2494),\n",
       " ('people', 1969),\n",
       " ('think', 1803),\n",
       " ('going', 1613),\n",
       " ('president', 1400),\n",
       " ('would', 1371),\n",
       " ('one', 1099),\n",
       " ('get', 1065),\n",
       " ('make', 1004),\n",
       " ('want', 957),\n",
       " ('know', 942),\n",
       " ('said', 917),\n",
       " ('country', 914),\n",
       " ('uh', 903),\n",
       " ('years', 855),\n",
       " ('got', 815),\n",
       " ('us', 766),\n",
       " ('well', 756),\n",
       " ('say', 734),\n",
       " ('tax', 704)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [(k, v) for k, v in word_counts.items()]\n",
    "top_n = sorted(\n",
    "    pairs, key=lambda item: -item[1]\n",
    ")\n",
    "top_n[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "composed-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordmap = generate_wordmap(list(word_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "endangered-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: dropout?\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dims):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dims, 500)\n",
    "        self.layer2 = nn.Linear(500, 75)\n",
    "        self.layer3 = nn.Linear(75, 3)\n",
    "\n",
    "    # Called on each input\n",
    "    # Computes the outputs (and next hidden state)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = nn.Softmax(dim=3)(x)\n",
    "        return x[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "running-relief",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3307, 0.3400, 0.3293]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net = NeuralNet(len(wordmap))\n",
    "neural_net(torch.Tensor([[[string_to_vector('president think say',wordmap)]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "recent-salem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3305, 0.3412, 0.3283]], grad_fn=<SelectBackward>), 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net(torch.Tensor([[[string_to_vector(train_set[0][1],wordmap)]]])), train_set[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dimensional-sitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.889\n",
      "[1,  4000] loss: 0.906\n",
      "Epoch 0\n",
      "CE Loss: 0.9027909636497498\n",
      "Macro F1: 0.26250378902697785\n",
      "[2,  2000] loss: 0.905\n",
      "[2,  4000] loss: 0.901\n",
      "Epoch 1\n",
      "CE Loss: 0.9023587703704834\n",
      "Macro F1: 0.26250378902697785\n",
      "[3,  2000] loss: 0.889\n",
      "[3,  4000] loss: 0.908\n",
      "Epoch 2\n",
      "CE Loss: 0.9021825194358826\n",
      "Macro F1: 0.26250378902697785\n",
      "[4,  2000] loss: 0.908\n",
      "[4,  4000] loss: 0.894\n",
      "Epoch 3\n",
      "CE Loss: 0.902087390422821\n",
      "Macro F1: 0.26250378902697785\n",
      "[5,  2000] loss: 0.897\n",
      "[5,  4000] loss: 0.900\n",
      "Epoch 4\n",
      "CE Loss: 0.9020254611968994\n",
      "Macro F1: 0.26250378902697785\n",
      "[6,  2000] loss: 0.888\n",
      "[6,  4000] loss: 0.899\n",
      "Epoch 5\n",
      "CE Loss: 0.9019823670387268\n",
      "Macro F1: 0.26250378902697785\n",
      "[7,  2000] loss: 0.897\n",
      "[7,  4000] loss: 0.898\n",
      "Epoch 6\n",
      "CE Loss: 0.9019449353218079\n",
      "Macro F1: 0.26250378902697785\n",
      "[8,  2000] loss: 0.891\n",
      "[8,  4000] loss: 0.900\n",
      "Epoch 7\n",
      "CE Loss: 0.9019091725349426\n",
      "Macro F1: 0.26250378902697785\n",
      "[9,  2000] loss: 0.896\n",
      "[9,  4000] loss: 0.892\n",
      "Epoch 8\n",
      "CE Loss: 0.9018746614456177\n",
      "Macro F1: 0.26250378902697785\n",
      "[10,  2000] loss: 0.894\n",
      "[10,  4000] loss: 0.898\n",
      "Epoch 9\n",
      "CE Loss: 0.901832640171051\n",
      "Macro F1: 0.26250378902697785\n",
      "[11,  2000] loss: 0.894\n",
      "[11,  4000] loss: 0.900\n",
      "Epoch 10\n",
      "CE Loss: 0.9017780423164368\n",
      "Macro F1: 0.26250378902697785\n",
      "[12,  2000] loss: 0.895\n",
      "[12,  4000] loss: 0.903\n",
      "Epoch 11\n",
      "CE Loss: 0.9017056822776794\n",
      "Macro F1: 0.26250378902697785\n",
      "[13,  2000] loss: 0.894\n",
      "[13,  4000] loss: 0.897\n",
      "Epoch 12\n",
      "CE Loss: 0.90159010887146\n",
      "Macro F1: 0.26250378902697785\n",
      "[14,  2000] loss: 0.895\n",
      "[14,  4000] loss: 0.896\n",
      "Epoch 13\n",
      "CE Loss: 0.9013891816139221\n",
      "Macro F1: 0.26250378902697785\n",
      "[15,  2000] loss: 0.897\n",
      "[15,  4000] loss: 0.901\n",
      "Epoch 14\n",
      "CE Loss: 0.9009678363800049\n",
      "Macro F1: 0.26250378902697785\n",
      "[16,  2000] loss: 0.898\n",
      "[16,  4000] loss: 0.889\n",
      "Epoch 15\n",
      "CE Loss: 0.8997184634208679\n",
      "Macro F1: 0.26250378902697785\n",
      "[17,  2000] loss: 0.898\n",
      "[17,  4000] loss: 0.889\n",
      "Epoch 16\n",
      "CE Loss: 0.8903211355209351\n",
      "Macro F1: 0.26250378902697785\n",
      "[18,  2000] loss: 0.876\n",
      "[18,  4000] loss: 0.857\n",
      "Epoch 17\n",
      "CE Loss: 0.8416733145713806\n",
      "Macro F1: 0.4149171553692918\n",
      "[19,  2000] loss: 0.828\n",
      "[19,  4000] loss: 0.812\n",
      "Epoch 18\n",
      "CE Loss: 0.8173429369926453\n",
      "Macro F1: 0.46221361243547027\n",
      "[20,  2000] loss: 0.802\n",
      "[20,  4000] loss: 0.798\n",
      "Epoch 19\n",
      "CE Loss: 0.8088001012802124\n",
      "Macro F1: 0.4749212799692539\n",
      "[21,  2000] loss: 0.787\n",
      "[21,  4000] loss: 0.784\n",
      "Epoch 20\n",
      "CE Loss: 0.8075636029243469\n",
      "Macro F1: 0.4750687070246918\n",
      "[22,  2000] loss: 0.775\n",
      "[22,  4000] loss: 0.782\n",
      "Epoch 21\n",
      "CE Loss: 0.8034546971321106\n",
      "Macro F1: 0.4846867037056593\n",
      "[23,  2000] loss: 0.771\n",
      "[23,  4000] loss: 0.772\n",
      "Epoch 22\n",
      "CE Loss: 0.8040685057640076\n",
      "Macro F1: 0.48655989595148436\n",
      "[24,  2000] loss: 0.762\n",
      "[24,  4000] loss: 0.768\n",
      "Epoch 23\n",
      "CE Loss: 0.8045849204063416\n",
      "Macro F1: 0.4888572783309626\n",
      "[25,  2000] loss: 0.755\n",
      "[25,  4000] loss: 0.761\n",
      "Epoch 24\n",
      "CE Loss: 0.8085472583770752\n",
      "Macro F1: 0.48780113395080854\n",
      "[26,  2000] loss: 0.754\n",
      "[26,  4000] loss: 0.754\n",
      "Epoch 25\n",
      "CE Loss: 0.8057283163070679\n",
      "Macro F1: 0.4860757955494798\n",
      "[27,  2000] loss: 0.752\n",
      "[27,  4000] loss: 0.750\n",
      "Epoch 26\n",
      "CE Loss: 0.809124231338501\n",
      "Macro F1: 0.47702807370915945\n",
      "[28,  2000] loss: 0.742\n",
      "[28,  4000] loss: 0.753\n",
      "Epoch 27\n",
      "CE Loss: 0.8074567317962646\n",
      "Macro F1: 0.48280620705993843\n",
      "[29,  2000] loss: 0.740\n",
      "[29,  4000] loss: 0.749\n",
      "Epoch 28\n",
      "CE Loss: 0.8076235055923462\n",
      "Macro F1: 0.4833727966619465\n",
      "[30,  2000] loss: 0.738\n",
      "[30,  4000] loss: 0.744\n",
      "Epoch 29\n",
      "CE Loss: 0.8069255352020264\n",
      "Macro F1: 0.4834762315763515\n",
      "[31,  2000] loss: 0.733\n",
      "[31,  4000] loss: 0.747\n",
      "Epoch 30\n",
      "CE Loss: 0.8066991567611694\n",
      "Macro F1: 0.4813654149448005\n",
      "[32,  2000] loss: 0.743\n",
      "[32,  4000] loss: 0.730\n",
      "Epoch 31\n",
      "CE Loss: 0.8060514330863953\n",
      "Macro F1: 0.48501737875604634\n",
      "[33,  2000] loss: 0.735\n",
      "[33,  4000] loss: 0.731\n",
      "Epoch 32\n",
      "CE Loss: 0.8048658967018127\n",
      "Macro F1: 0.48524735953125536\n",
      "[34,  2000] loss: 0.729\n",
      "[34,  4000] loss: 0.736\n",
      "Epoch 33\n",
      "CE Loss: 0.8076598048210144\n",
      "Macro F1: 0.4816990944877843\n",
      "[35,  2000] loss: 0.726\n",
      "[35,  4000] loss: 0.730\n",
      "Epoch 34\n",
      "CE Loss: 0.8070673942565918\n",
      "Macro F1: 0.4811173340941055\n",
      "[36,  2000] loss: 0.732\n",
      "[36,  4000] loss: 0.727\n",
      "Epoch 35\n",
      "CE Loss: 0.8099004030227661\n",
      "Macro F1: 0.4834099509022109\n",
      "[37,  2000] loss: 0.730\n",
      "[37,  4000] loss: 0.733\n",
      "Epoch 36\n",
      "CE Loss: 0.8079261183738708\n",
      "Macro F1: 0.4829419373030759\n",
      "[38,  2000] loss: 0.725\n",
      "[38,  4000] loss: 0.724\n",
      "Epoch 37\n",
      "CE Loss: 0.8090229034423828\n",
      "Macro F1: 0.47813145885771585\n",
      "[39,  2000] loss: 0.720\n",
      "[39,  4000] loss: 0.722\n",
      "Epoch 38\n",
      "CE Loss: 0.8130359649658203\n",
      "Macro F1: 0.4803365851270043\n",
      "[40,  2000] loss: 0.724\n",
      "[40,  4000] loss: 0.725\n",
      "Epoch 39\n",
      "CE Loss: 0.8091930747032166\n",
      "Macro F1: 0.4804416892490759\n",
      "[41,  2000] loss: 0.728\n",
      "[41,  4000] loss: 0.719\n",
      "Epoch 40\n",
      "CE Loss: 0.8108060359954834\n",
      "Macro F1: 0.4830337362653701\n",
      "[42,  2000] loss: 0.714\n",
      "[42,  4000] loss: 0.724\n",
      "Epoch 41\n",
      "CE Loss: 0.8105453848838806\n",
      "Macro F1: 0.47910325255018194\n",
      "[43,  2000] loss: 0.722\n",
      "[43,  4000] loss: 0.717\n",
      "Epoch 42\n",
      "CE Loss: 0.8133801817893982\n",
      "Macro F1: 0.4795721440355188\n",
      "[44,  2000] loss: 0.721\n",
      "[44,  4000] loss: 0.719\n",
      "Epoch 43\n",
      "CE Loss: 0.8105600476264954\n",
      "Macro F1: 0.48025448823365413\n",
      "[45,  2000] loss: 0.722\n",
      "[45,  4000] loss: 0.713\n",
      "Epoch 44\n",
      "CE Loss: 0.8101081252098083\n",
      "Macro F1: 0.4805369626961209\n",
      "[46,  2000] loss: 0.718\n",
      "[46,  4000] loss: 0.718\n",
      "Epoch 45\n",
      "CE Loss: 0.8106468915939331\n",
      "Macro F1: 0.4797375784233802\n",
      "[47,  2000] loss: 0.720\n",
      "[47,  4000] loss: 0.714\n",
      "Epoch 46\n",
      "CE Loss: 0.8098169565200806\n",
      "Macro F1: 0.481714902385459\n",
      "[48,  2000] loss: 0.716\n",
      "[48,  4000] loss: 0.714\n",
      "Epoch 47\n",
      "CE Loss: 0.8104580044746399\n",
      "Macro F1: 0.4792348891048017\n",
      "[49,  2000] loss: 0.715\n",
      "[49,  4000] loss: 0.712\n",
      "Epoch 48\n",
      "CE Loss: 0.8086373805999756\n",
      "Macro F1: 0.4835819701786692\n",
      "[50,  2000] loss: 0.709\n",
      "[50,  4000] loss: 0.719\n",
      "Epoch 49\n",
      "CE Loss: 0.8086923360824585\n",
      "Macro F1: 0.48259778332242104\n"
     ]
    }
   ],
   "source": [
    "FORCE_OVERFIT = False\n",
    "\n",
    "if FORCE_OVERFIT:\n",
    "    so_called_train_set = train_set[:4]\n",
    "    epochs = 100\n",
    "else:\n",
    "    so_called_train_set = train_set\n",
    "    epochs = 50\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(neural_net.parameters(), lr=0.001, momentum=0.9)\n",
    "train_loader = torch.utils.data.DataLoader(so_called_train_set, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        _, x, labels, _ = data\n",
    "        \n",
    "        x = torch.FloatTensor([[list(map(\n",
    "            lambda string: string_to_vector(string, wordmap),\n",
    "            x\n",
    "        ))]])\n",
    "        \n",
    "        labels = list(map(\n",
    "            lambda x: x+1,\n",
    "            labels\n",
    "        ))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = neural_net(x)\n",
    "        loss = criterion(outputs, torch.LongTensor(labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    print(f'Epoch {epoch}')\n",
    "    validate(neural_net, val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "elder-istanbul",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss: 0.8086923360824585\n",
      "Macro F1: 0.48259778332242104\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def validate(model, val_set):\n",
    "    val_x = []\n",
    "    val_y_labels = []\n",
    "    for val_datum in val_set:\n",
    "        val_x.append(val_datum[1])\n",
    "        val_y_labels.append(val_datum[2]+1)\n",
    "    val_x = torch.FloatTensor([[list(map(\n",
    "        lambda string: string_to_vector(string, wordmap),\n",
    "        val_x\n",
    "    ))]])\n",
    "    val_outputs = neural_net(val_x)\n",
    "    loss = criterion(val_outputs, torch.LongTensor(val_y_labels))\n",
    "    print(f'CE Loss: {loss.item()}')\n",
    "    \n",
    "    val_output_labels = list(map(\n",
    "        lambda v_o: np.argmax(v_o.detach().numpy()),\n",
    "        val_outputs\n",
    "    ))\n",
    "    print(f'Macro F1: {metrics.f1_score(val_y_labels, val_output_labels, average=\"macro\")}')\n",
    "        \n",
    "validate(neural_net, val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-estimate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
