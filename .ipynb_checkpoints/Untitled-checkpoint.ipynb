{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complicated-representative",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-684d8eebf3fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpdb\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_trace\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'punkt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\github\\make-my-story\\env\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    777\u001b[0m                 )\n\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[1;31m# Error messages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\github\\make-my-story\\env\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;31m# Look up the requested collection or package.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_or_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Error loading %s: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\github\\make-my-story\\env\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m_info_or_id\u001b[1;34m(self, info_or_id)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_info_or_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_or_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0minfo_or_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\github\\make-my-story\\env\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36minfo\u001b[1;34m(self, id)\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \"\"\"Return the ``Package`` or ``Collection`` record for the\n\u001b[0;32m   1010\u001b[0m            given item.\"\"\"\n\u001b[1;32m-> 1011\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\github\\make-my-story\\env\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m_update_index\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[1;31m# Download the index file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m         self._index = nltk.internals.ElementWrapper(\n\u001b[1;32m--> 954\u001b[1;33m             \u001b[0mElementTree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m         )\n\u001b[0;32m    956\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python3.6\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python3.6\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python3.6\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[1;32m--> 544\u001b[1;33m                                   '_open', req)\n\u001b[0m\u001b[0;32m    545\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python3.6\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python3.6\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1390\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[1;32m-> 1392\u001b[1;33m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python3.6\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m         \u001b[1;31m# will parse host:port\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1317\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhttp_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhttp_conn_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1318\u001b[0m         \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_debuglevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_debuglevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python3.6\\lib\\http\\client.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, host, port, key_file, cert_file, timeout, source_address, context, check_hostname)\u001b[0m\n\u001b[0;32m   1423\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcert_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcert_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1425\u001b[1;33m                 \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mssl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_default_https_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1426\u001b[0m             \u001b[0mwill_verify\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverify_mode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mssl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCERT_NONE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcheck_hostname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python3.6\\lib\\ssl.py\u001b[0m in \u001b[0;36mcreate_default_context\u001b[1;34m(purpose, cafile, capath, cadata)\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[1;31m# OP_CIPHER_SERVER_PREFERENCE, OP_SINGLE_DH_USE and OP_SINGLE_ECDH_USE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[1;31m# by default.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m     \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSSLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPROTOCOL_TLS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpurpose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPurpose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSERVER_AUTH\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python3.6\\lib\\ssl.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, protocol, *args, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__new__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPROTOCOL_TLS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_SSLContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0m_SSLv2_IF_EXISTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ciphers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_DEFAULT_CIPHERS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.corpus\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from pdb import set_trace as st\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "'''\n",
    "stopwords = stopwords + [\n",
    "    \"n't\",\n",
    "    'not',\n",
    "    'mr',\n",
    "    'mr.'\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('train.csv', newline='', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, dialect='excel')\n",
    "    train_set = list(reader)\n",
    "\n",
    "train_set = list(map(\n",
    "    lambda x: [int(x[0]), x[1], int(x[2])],\n",
    "    train_set[1:]\n",
    "))\n",
    "\n",
    "random.shuffle(train_set)\n",
    "\n",
    "for datum in train_set:\n",
    "    if datum[2] == -1:\n",
    "        datum.append([1,0,0])\n",
    "    elif datum[2] == 0:\n",
    "        datum.append([0,1,0])\n",
    "    else:\n",
    "        datum.append([0,0,1])\n",
    "\n",
    "val_set = train_set[:2000]\n",
    "train_set = train_set[2000:]\n",
    "\n",
    "with open('test.csv', newline='', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, dialect='excel')\n",
    "    test_set = list(reader)\n",
    "\n",
    "test_set = list(map(\n",
    "    lambda x: (int(x[0]), x[1]),\n",
    "    test_set[1:]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_tokens(string):\n",
    "    tokens = word_tokenize(string)\n",
    "    tokens = filter(\n",
    "        lambda x: re.match(re.compile(r\"^[A-Za-z'][A-Za-z'.]*$\"), x),\n",
    "        tokens\n",
    "    )\n",
    "    tokens = map(\n",
    "        lambda x: x.lower(),\n",
    "        tokens\n",
    "    )\n",
    "    tokens = filter(\n",
    "        lambda x: x not in stopwords and x[0] != \"'\",\n",
    "        tokens\n",
    "    )\n",
    "    return list(tokens)\n",
    "\n",
    "def remove_unknown_words(vocab, tokens):\n",
    "    return list(filter(\n",
    "        lambda x: x in vocab,\n",
    "        tokens\n",
    "    ))\n",
    "\n",
    "def generate_word_counts(train_set):\n",
    "    word_counts = {}\n",
    "    for train_datum in train_set:\n",
    "        for word in string_to_tokens(train_datum[1]):\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    return word_counts\n",
    "\n",
    "def remove_rare_words_from_word_counts(word_counts, threshold=5):\n",
    "    new_dict = {}\n",
    "    for word in word_counts.keys():\n",
    "        if word_counts[word] >= threshold:\n",
    "            new_dict[word] = word_counts[word]\n",
    "    return new_dict\n",
    "\n",
    "# returns a dict that maps words to integers. will be used for encoding text.\n",
    "def generate_wordmap(words):\n",
    "    wordmap = {}\n",
    "    for i in range(len(words)):\n",
    "        wordmap[words[i]] = i\n",
    "    return wordmap\n",
    "\n",
    "def string_to_vector(string, wordmap):\n",
    "    tokens = string_to_tokens(string)\n",
    "    tokens = remove_unknown_words(wordmap.keys(), tokens)\n",
    "    result = np.zeros(len(wordmap))\n",
    "    for token in tokens:\n",
    "        result[wordmap[token]] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = generate_word_counts(train_set)\n",
    "print(len(word_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = remove_rare_words_from_word_counts(word_counts)\n",
    "print(len(word_counts.keys()))\n",
    "plt.plot(sorted(word_counts.values(), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-garbage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs = [(k, v) for k, v in word_counts.items()]\n",
    "top_n = sorted(\n",
    "    pairs, key=lambda item: -item[1]\n",
    ")\n",
    "top_n[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordmap = generate_wordmap(list(word_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: dropout?\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dims):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dims, 500)\n",
    "        self.layer2 = nn.Linear(500, 75)\n",
    "        self.layer3 = nn.Linear(75, 3)\n",
    "\n",
    "    # Called on each input\n",
    "    # Computes the outputs (and next hidden state)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = nn.Softmax(dim=3)(x)\n",
    "        return x[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = NeuralNet(len(wordmap))\n",
    "neural_net(torch.Tensor([[[string_to_vector('president think say',wordmap)]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net(torch.Tensor([[[string_to_vector(train_set[0][1],wordmap)]]])), train_set[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_OVERFIT = False\n",
    "\n",
    "if FORCE_OVERFIT:\n",
    "    so_called_train_set = train_set[:4]\n",
    "    epochs = 100\n",
    "else:\n",
    "    so_called_train_set = train_set\n",
    "    epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(neural_net.parameters(), lr=0.001, momentum=0.9)\n",
    "train_loader = torch.utils.data.DataLoader(so_called_train_set, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        _, x, labels, _ = data\n",
    "        \n",
    "        x = torch.FloatTensor([[list(map(\n",
    "            lambda string: string_to_vector(string, wordmap),\n",
    "            x\n",
    "        ))]])\n",
    "        \n",
    "        labels = list(map(\n",
    "            lambda x: x+1,\n",
    "            labels\n",
    "        ))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = neural_net(x)\n",
    "        loss = criterion(outputs, torch.LongTensor(labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def validate(model, val_set):\n",
    "    val_x = []\n",
    "    val_y_labels = []\n",
    "    for val_datum in val_set:\n",
    "        val_x.append(val_datum[1])\n",
    "        val_y_labels.append(val_datum[2]+1)\n",
    "    val_x = torch.FloatTensor([[list(map(\n",
    "        lambda string: string_to_vector(string, wordmap),\n",
    "        val_x\n",
    "    ))]])\n",
    "    val_outputs = neural_net(val_x)\n",
    "    loss = criterion(val_outputs, torch.LongTensor(val_y_labels))\n",
    "    print(f'CE Loss: {loss.item()}')\n",
    "    \n",
    "    val_output_labels = list(map(\n",
    "        lambda v_o: np.argmax(v_o.detach().numpy()),\n",
    "        val_outputs\n",
    "    ))\n",
    "    print(f'Macro F1: {metrics.f1_score(val_y_labels, val_output_labels, average=\"macro\")}')\n",
    "        \n",
    "validate(neural_net, val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-estimate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
